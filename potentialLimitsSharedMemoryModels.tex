\section{Potential limits in shared memory programming models}

In this section two technical aspects that can potentially affect the overall performance of applications conforming to the shared memory programming model are evaluated. The first is related to the performance of the computer's memory sub-system, and the second relates to the cost of the barriers required by parallel program to ensure correct results.
%\medskip


\subsection*{Stream Benchmark}

For a long time we have witness how CPUs are being getting faster much more quickly than the computer's memory sub-system. Modern multi-core processors have become voracious consumers of data, and, as a consequence, more and more programs are being limited in performance by the memory bandwidth of the system, rather than by the computational performance of the CPU\cite{McCalpin2007}. It is, therefore, particularly important to be able to determine the memory bandwidth of the system at hand. 

\medskip


The STREAM benchmark\cite{McCalpin2007} is a suite of four (Copy, Scale, Add and Triad) simple tests that measures sustainable memory bandwidth (in MB/s) and the corresponding computation rate for simple vector kernels. In general, it is used to measure the evolution of the memory bandwidth of shared memory systems as more computing component are included in the test. Traditionally, the test has been implemented using OpenMP and the computing component are usually referred as threads.

\medskip

For MPI$_{sm}$ to become a viable alternative to OpenMP in the context of the shared memory programming model, it is fundamental that its sustainable memory bandwidth compares favorably to its OpenMP counterpart.

\medskip

The stream program that can be download from its web site \cite{McCalpin2007} is already annotated with directives (\textbf{\#pragma}) to make it a correct OpenMP program. This program was modified in order to develop an equivalent  MPI$_{sm}$ version. Both programs were run, using various compilers and MPI implementations. Some results, as well as lessons learned, follow.

\medskip

Figure \ref{fig:TriadTestBefore} presents results obtained on one system (\textbf{stout}) for the Triad test of the stream suite; results of the other tests (Copy, Scale and Add) are similar. The left plot, (\ref{fig:TriadBefore}) shows the actual bandwidth achieved while the right plot (\ref{fig:TriadRatioBefore}) show the MPI$_{sm}$ to OpenMP ratio obtained.
 
\medskip

\begin{figure} [h!]
    \centering
    \captionsetup{justification=centering, singlelinecheck=false}
    \begin{subfigure}{.6\textwidth}
      \hspace*{-1.5cm} 
      \includegraphics[width=0.95\linewidth]{Plots/streamBenchmark/porter-TriadBefore.pdf}
      \caption[]{Bandwidth per number of processes/threads.}
      \label{fig:TriadBefore}
    \end{subfigure}%
    \begin{subfigure}{.6\textwidth}
      \hspace*{-1.5cm} 
      \includegraphics[width=0.95\linewidth]{Plots/streamBenchmark/porter-TriadRatioBefore.pdf}
      \caption{Ratio MPI$_{sm}$ to OpenMP.}
      \label{fig:TriadRatioBefore}
    \end{subfigure}
    \caption{Triad test. No binding policy for OpenMP}
\label{fig:TriadTestBefore}
\end{figure}


In Figure \ref{fig:TriadBefore} the segmented lines correspond to OpenMP while the continuous lines represent MPI$_{sm}$. Notice the extremely irregular behavior shown by all the OpenMP cases. Exploring the causes of such behavior lead to the first lesson in this section: \textbf{thread placement}. As mentioned in the Introduction, all the systems used in this study corresponds to systems having more than one socket. In this kind of systems, application having multiple threads could run into affinity problems.

\medskip

The lack of a \emph{by-default} thread affinity control policy of all the compilers used in the study results the erratic behavior shown in the Figure \ref{fig:TriadBefore}, because the threads are allow to be moved, by the OS, not only to a different core but also to a different socket. On the other hand, the more stable behavior that can be observed for the MPI$_{sm}$ case is due to the fact that the MPI implementations used \emph{do} have a \emph{by-default} binding policy for the processes running in the node.

\medskip

The ways to control thread affinity in OpenMP used to be compiler-dependent. However, OpenMP version 4.0 specification introduced a standardized mean to control thread affinity allowing, for the first time, an uniform way to have fine grain control over the affinity policy to compilers conforming to the standard. It introduced two new concepts: binding policy and place partition.

The binding policy determines if the threads will be bound and how they will be distributed, while the place partition is the set of places to which threads can be bound. One simple way of control the binding policy is by setting the \textbf{\texttt{OMP\_PROC\_BIND}} environment variable. Similarly, a way of control the place partition is by setting the \textbf{\texttt{OMP\_PLACES}} environment variable. 

\medskip

In our case, we found convenient to set 

\begin{itemize} 

\item \textbf{\texttt{OMP\_PROC\_BIND}}=spread 

\item \textbf{\texttt{OMP\_PLACES}}=sockets

\end{itemize}

meaning that the threads will be bound and distributed (or spread) among the sockets. More details about the values allowed for these enviromantal variables can be found in the current OpenMP API specification.

\medskip

Once a thread policy was set for the OpenMP cases, the stream tests were run again and the results are shown in Figure \ref{fig:TriadTest}. As before, the left plot, (\ref{fig:Triad}) shows the actual bandwidth achieved while the right plot (\ref{fig:TriadRatio}) show the MPI$_{sm}$ to OpenMP ratio obtained. 

\medskip
From the results shown in Figure \ref{fig:TriadTest} it can be seen that the sustainable memory bandwidth achievable by MPI$_{sm}$ is effectively at pair with its OpenMP counterpart.


\begin{figure} [h!]
    \centering
    \captionsetup{justification=centering, singlelinecheck=false}
    \begin{subfigure}{.6\textwidth}
      \centering
      \hspace*{-1.5cm} 
      \includegraphics[width=0.95\linewidth]{Plots/streamBenchmark/porter-Triad.pdf}
      \caption[]{Bandwidth per number of processes/threads.}
      \label{fig:Triad}
    \end{subfigure}%
    \begin{subfigure}{.6\textwidth}
      \centering
      \hspace*{-1.5cm} 
      \includegraphics[width=0.95\linewidth]{Plots/streamBenchmark/porter-TriadRatio.pdf}
      \caption{Ratio MPI$_{sm}$ to OpenMP.}
      \label{fig:TriadRatio}
    \end{subfigure}
\caption{Triad test.}
\label{fig:TriadTest}
\end{figure}

\newpage

\subsection*{Barriers}
%causes a central processing unit (CPU) or compiler to enforce 

A memory barrier is a type of instruction that requires an ordering constraint on memory operations issued before and after it. They are necessary because most modern CPUs employ performance optimizations that can result in out-of-order execution. This reordering of memory operations (loads and stores) can produce incorrect results in parallel programs unless carefully controlled.

\medskip


The efficiency of the barrier implementation could potentially affect the overall performance of any parallel application, but particularly those conforming to the shared memory programming model. Therefore an effort was made to estimate, as accurate as possible, the time delay produced by the barrier itself. A large difference in the barrier delay between MPI$_{sm}$ and OpenMP could result determinant in choosing one or the other.

\medskip

Notice that what is being measured is not the time a particular thread/process takes between entering and exiting
the barrier because that time includes the time spend in the barrier waiting for its partners to arrive. Instead, the parameter being measured correspond to the time between the last thread/process entering the barrier to when all of them exit the barrier\cite{Schmidt_2012}, and could be approximated by the minimum time taken by all the thread/process between entering and exiting the barrier. Also notice that the parameter being measured is rather small and, therefore, its value is better represented by an average taken over a relatively large amount of measurements.


\medskip

Figure \ref{fig:PseudoCode3} presents a pseudo code of the algorithm used to determine the cost of the barrier in the MPI$_{sm}$ case; the algorithm for OpenMP case is essentially the same and, therefore, is not presented here. In lines 1-2 some variables are declared and initialized. Lines 3-13 consist of a for loop used to generate a sufficiently large amount of measurements required by an accurate result. Line 4 contains a barrier used to synchronize all the processes before performing the actual measurement. In lines 6 and 8 start and stop the clock, while line 7 contains the barrier being timed. In line 10, each process gets the time it spends in the barrier which, in general, includes a delay due to waiting for partners to arrive. In lines 11-12, the smallest time among all the processes is obtained and accumulated in the \textbf{\texttt{barrier\_time}} variable. Finally, in line 14 an average for the barrier time is obtained.



\begin{figure} [t!]
\centering
\captionsetup{justification=centering, singlelinecheck=false}
\begin{lstlisting}[style=CStyle]
uint64_t max_iterations = 1000000L;
uint64_t time, barrier_time=0;
for (uint64_t n=0 ; n<max_iterations; ++n) {
    MPI_Barrier(sm_comm);
        
    clock_gettime(CLOCK_MONOTONIC, &start);
    MPI_Barrier(sm_comm);    
    clock_gettime(CLOCK_MONOTONIC, &end);
    
    time = 1000000000L * (end.tv_sec - start.tv_sec) + end.tv_nsec - start.tv_nsec;
    MPI_Allreduce( MPI_IN_PLACE, &time, 1, MPI_UNSIGNED_LONG, MPI_MIN,sm_comm);
    barrier_time+=time;
}	// end for //
barrier_time/=(max_iterations);
\end{lstlisting}    
\caption{Pseudo used to estimate MPI$_{sm}$ barrier time.}
\label{fig:PseudoCode3}
\end{figure}


\medskip

Figure \ref{fig:BarrierAndErrorBars} shows result obtained on one system (stout). The left plot \ref{fig:Barrier} presents average barrier times, in nano-seconds, and the right plot \ref{fig:BarrierErrorBars} shows the variability of those measurement.

\medskip

Figure \ref{fig:Barrier} presents some contradictory results. While for the Gnu compiler the barrier time taken by OpenMP is larger than its corresponding MPI$_{sm}$ counterpart for any number of processes/threads, for the other two compilers (Intel and PGI) the opposite is true. 

\medskip

Figure \ref{fig:BarrierErrorBars} allows to see that the variability of the obtained results is relatively small.

\medskip

The results shown here for the stout system are similar to results obtained in the other systems used in this study.


\begin{figure} [h!]
    \centering
    \captionsetup{justification=centering, singlelinecheck=false}
    \begin{subfigure}{.6\textwidth}
      \centering
      \hspace*{-1.5cm} 
      \includegraphics[width=0.95\linewidth]{Plots/barrier/stout.pdf}
      \caption[]{Estimated time.}
      \label{fig:Barrier}
    \end{subfigure}%
    \begin{subfigure}{.6\textwidth}
      \centering
      \hspace*{-1.5cm} 
      \includegraphics[width=0.95\linewidth]{Plots/barrier/stoutError.pdf}
      \caption{Errorbars to show variability of results.}
      \label{fig:BarrierErrorBars}
    \end{subfigure}
\caption{Barrier Results.}
\label{fig:BarrierAndErrorBars}
\end{figure}





\begin{comment}


Two identical versions of the stream benchmarks one using  and 
 were created and it 
 results are compared below for several systems. 
 
 
 However, the arrays 



 capabilities of the 

an MPI$_{sm}$ version of the program was developed and used to compare its performance to that obtained using its 



OpenMP versions of the STREAM benchmark program has been used to measure the evolution of the sustainable memory bandwidth of shared memory system as more threads are included in the test. 




What is STREAM?
The STREAM benchmark is a simple synthetic benchmark program that measures sustainable memory bandwidth (in MB/s) and the corresponding computation rate for simple vector kernels. 

Why should I care?

Computer cpus are getting faster much more quickly than computer memory systems. 

As this progresses, 

As an extreme example, several current high-end machines run simple arithmetic kernels for out-of-cache operands at 4-5 percent of their rated peak speeds --- that means that they are spending 95-96 percent of their time idle and waiting for cache misses to be satisfied.

The STREAM benchmark is specifically designed to work with datasets much larger than the available cache on any given system, so that the results are (presumably) more indicative of the performance of very large, vector style applications.

If you want more words, I have written a paper on STREAM: Sustainable Memory Bandwidth in Current High Performance Computers

A somewhat broader look on the issue, see my paper: Memory Bandwidth and Machine Balance in Current High Performance Computers. STREAM is also a useful component of models for scaling of homogeneous throughput workloads (like the SPEC CPU "rate" benchmarks). 

Examples of models based on STREAM measurements that do a pretty good job of estimating SPECfp rate2000 scaling are included in several presentations.

\end{comment}

