\section{MPI Shared Memory (MPI$_{sm}$)}

This section lists and briefly describes the functions and data types used in writing MPI$_{sm}$ programs. Some of these functions have existed in MPI for years, while others have recently been added to the standard. Refer to the examples in the following sections or the MPI standard for a more detailed description.

\subsubsection*{Memory Window Data Type}

The first steps in using Remote Memory Access (RMA) is to define the memory that will be available for remote memory operations. The memory in a process that can be accessed by another process through the use of RMA routines is called a \emph{memory window}. MPI defines a data type, \textbf{MPI\_Win}, to be used as a handle in order to create and destroy memory windows. In a similar way, MPI$_{sm}$ also uses this data type to dynamically allocate portions of shared memory to be used by processes within a node. Additionally, the \textbf{MPI\_Win} data type is also used to free the dynamically allocated shared memory when it is not longer needed.


\subsubsection*{Shared Memory Communicator}

Before being able to allocate shared memory, the processors belonging to the main communicator (\textbf{MPI\_COMM\_WORLD}) must be separated into sub-communicators of processors existing inside a single node. This is achieved by using the \textbf{MPI\_Comm\_split\_type()} function.

\subsubsection*{Shared Memory Allocation}

Once the sub-communicator of processors existing inside a single node has been created, it can be used to allocate shared memory using the new \textbf{MPI\_Win\_allocate\_shared()} function. This function returns a process-local pointer containing the address of the shared memory allocated by the calling process.

\subsubsection*{Shared Memory Query}

The address of the allocated shared memory segment is meaningful only to the calling process (and should not be communicated), therefore, another function, \textbf{MPI\_Win\_shared\_query()}, can be used to retrieve the local address of the shared memory on other processors.

\subsubsection*{Synchronization Techniques}

This is a behavior that MPI$_{sm}$ models after RMA data transfers. There are two basic methods used by MPI to complete data transfers (meaning when the data is ready to be used by the receiving process): active and passive target synchronization. Active synchronization involves the use of the \textbf{MPI\_Win\_fence()} function while passive synchronization used a combination of the \textbf{MPI\_Win\_lock()}, \textbf{MPI\_Win\_sync()} ,  \textbf{MPI\_Barrier()} and  \textbf{MPI\_Win\_unlock()} functions. Since passive synchronization is regarded as an advanced technique and because it has been shown to outperform active synchronization, it was the synchronization scheme used in the results presented in the following sections.

\subsubsection*{Shared Memory Deallocation}
When the shared memory is not longer needed, it should be freed with a call to \textbf{MPI\_Win\_free()}.


\subsubsection*{Pseudo Code}

The following pseudo code provides an illustration of the sequence required to use the MPI$_{sm}$ programing mode. The code presented below uses the passive synchronization technique previously described.

In line 6 a variable of type MPI\_Win is declared. In line 11 a shared memory communicator is created. Size and ranks for the processors belonging to the shared communicator are determined in lines 14 and 15. In line 18  shared memory are allocated. In line 27, just before the shared memory is utilized, the MPI\_Win\_lock\_all() is called. In line 23 a function is called to initialize the shared memory. In line 25 and 26, just before the shared memory can be utilized after initialization,calls to MPI\_Win\_sync() and MPI\_Barrirer() are done to ensure that the data is available. When the buffer containing the shared memory is not longer being modified, a call to the function MPI\_Unlock\_all() is required as shown in line 28. Finally the shared memory can be deallocated as shown in line 30.

\begin{lstlisting}[style=CStyle]
#include <stdlib.h>
#include <stdio.h>

int main(int argc, char *argv[])
{
    MPI_Win sm_winT;
    MPI_Aint sz;
    int dispUnit;
    .
    MPI_Comm sm_comm;
    MPI_Comm_split_type(MPI_COMM_WORLD,MPI_COMM_TYPE_SHARED, 0,MPI_INFO_NULL, &sm_comm);

    int mySharedRank,sharedSize;
    MPI_Comm_rank(sm_comm,&mySharedRank);
    MPI_Comm_size(sm_comm,&sharedSize);

    double *Temperature;
    MPI_Win_allocate_shared((MPI_Aint) size*sizeof(double),
                            sizeof(double),MPI_INFO_NULL,sm_comm,
                            &Temperature,&sm_winT);
    MPI_Win_shared_query(sm_winT, mySharedRank, &sz,&dispUnit,&Temperature_last);
    .
    MPI_Win_lock_all(0,sm_winT);
    initialize(Temperature,sRow, eRow, myWorldRank,worldSize-1);
    MPI_Win_sync(sm_winT);
    MPI_Barrier(sm_comm);
    .
    MPI_Win_unlock_all(sm_winT);
    .
    MPI_Win_free(&sm_winT);
    MPI_Finalize();
    return 0;
} // end main() //
\end{lstlisting}



\begin{comment}

\begin{outline}
    \item {\bf MPI functions for shared memory }\\
      Description and use of the most relevant functions for shared memory introduced in MPI-3
    \item {\bf Shared Memory Allocation} \\
    Contiguous versus Noncontiguous memory allocation schemes\\
    Advantages and disadvantages\\
    Recommendations
    \item {\bf Relationship to Remote Memory Access }
      \begin{outline}
        \item {\bf Remote Memory Access Windows}
      \end{outline}
    \item {\bf Synchronization Techniques} \\
    Difference between $\textbf{MPI\_Win\_fence()}$ and $\textbf{MPI\_Win\_sync()}$

\end{outline}

\end{comment}

