\section{Introduction} \label{introduction}
%Although 
The message passing interface (MPI) has been the predominant parallel programming model since the mid-1900. It constitutes the base of the \emph{distributed} programming model in which components located on networked computers communicate and coordinate their actions by passing messages. Contrastingly, in a \emph{shared-memory} programming model, parallel processes share a global memory address space that they read and write to asynchronously. 

Today's clusters often comprise many nodes connected by a fast network each having processors that contain multiple cores. The current trend in cluster architecture shows an increasing number of cores per node, but also a declining memory per core. In traditional MPI programming, when a process is run on each of the n cores, for performance efficiency the data is often replicated at each process causing the memory demands of the application to grow linearly with the number of processes. Fortunately, the hardware often supports direct memory loads and stores, so these cores can be programmed to share part of the node's memory, potentially improving performance by reducing memory motion and footprint\cite{UsingAdvancedMPI}. 



\subsection*{Programming for Multicore}

The evolution towards multicore architecture has also force a shift in the traditional use of the \emph{distributed} programming model, in which all the communication between processes is handled essentially by the send/receive family of MPI functions, towards approaches that combines message passing with shared-memory techniques, often called \textbf{hybrid programming}\cite{UsingAdvancedMPI}. In this hybrid model, the threads or processes within a node communicate directly with each other by sharing the node's memory, while the inter-node communication is indeed handled using the send/receive traditional approach. One of the most common choice (among many) for hybrid programming, combines MPI for communication across nodes and OpenMP to share memory within a node. This approach is often referred as MPI+OpenMP.

\medskip

The MPI-3 standard added several functions to allow processes that exist within a node to directly allocate and share the node's memory, in a way similar to OpenMP. This new MPI Shared Memory model (MPI$_{sm}$) allows another approach to hybrid programming in which, traditional MPI is still used for inter-node communication while MPI$_{sm}$ is used for the intra-node communication through shared memory. In this document, this hybrid model is denoted as MPI+MPI$_{sm}$ to distinguish it from MPI+OpenMP. Brinskiy\cite{brinskiy2015} affirms that MPI$_{sm}$ can be used to incrementally change existing MPI codes in order to accelerate communication between processes on the shared-memory nodes.

\medskip

This MPI Shared Memory model, if proven to be efficient, could in turn become a viable alternative to OpenMP as a shared memory programming model.
 
\medskip

The objective of this guide is to describe, via examples, the shared memory capabilities introduced by the MPI-3 standard, and to explore its viability in the context of both the \textbf{share memory programming model} and as a \textbf{hybrid programming model}.


\subsection*{Systems and Compilers used}

All the computer utilized in this study corresponds to systems having more than one socket (ccNUMA nodes). These kind of architecture is ubiquitous nowadays. On those systems all the memory is accessible by all the cores, however the memory is distributed between the sockets. The efficient programming of these systems requires awareness of issues such as memory effects and thread/process placement.


Three compilers were used in the results presented in the following sections: the GNU \textbf{gcc}  version 7.3.0 (OpenMP 4.5), the Intel \textbf{icc} version 18.0.1 (OpenMP 5.0) and the  PGI \textbf{pgcc} version 18.3 (OpenMP 4.0). For the MPI implementation used, \textbf{gcc} and the \textbf{pgi} compliers both use \textbf{open-mpi} while the \textbf{intel} compiler used it own mpi implementation.



\subsection*{Organization}


\medskip

The lessons learned during the development of this work will be remarked and commented in the coming sections. This document is organized as follows:

\begin{itemize} 
\item Section 2 describes a small set of functions and techniques required to use MPI$_{sm}$ as a shared memory and as hybrid programming model.

\item Section 3 describes some technical aspects that affects the performance of both MPI$_{sm}$ and OpenMP.

\item Section 4 presents and compare results of the Stream benchmark obtained using MPI$_{sm}$ and OpenMP.

\item Section 5 compares the performance of MPI$_{sm}$ and OpenMP.

\item Section 6 compares the performance of two hybrid programming models: MPI+MPI$_{sm}$ and MPI+OpenMP.

\item Section 7 list a set of pros and cons of MPI$_{sm}$ as a shared memory and as hybrid programming model.

\item Section 8 contains a summary and conclusions.


%\item Section 5 describe some additional functions and techniques required by hybrid programming. Additionally it compares two hybrid programming model: MPI+MPI$_{sm}$ and MPI+OpenMP.

\end{itemize}








\begin{comment}






Today's trend of having 
an increasing number of cores per CPU (computing component, socket,chip) have created the need for approaches that combines the best of this two programming models: 


Almost all chips are multicore these days


 into what is nowadays known as Hybrid programming models.


It describes parallelism between processes having separate memory space; 

a 


in contrast to 

\emph{thread} parallelism which provides a shared memory model within a process. 

OpenMP and Pthreads are common examples of the \emph{thread} parallelism model.




Scale of machines to come encourage the use of different programming models to address issues such as

\begin{itemize} 

\item Declining memory per core

\item Multiple threads/core

\item Load balance

\item Algorithmic issues


\end{itemize}
\end{comment}


