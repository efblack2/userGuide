\section{Introduction}
%Although 
The message passing interface (MPI) has been the predominant parallel programing model since the mid-1900. It constitutes the base of the \emph{distributed} programming model in which components located on networked computers communicate and coordinate their actions by passing messages. Contrastingly, in a \emph{shared-memory} programming model, parallel processes share a global memory address space that they read and write to asynchronously. 

Today's clusters often comprise multiple cores per node sharing memory, and the nodes themselves are connected by a fast network. The current trends shows an increasing number of cores per node as well as a declining memory per core.

\subsection*{Programming for Multicore}

The traditional use of the \emph{distributed} programming model, in which all the communication between processes is handled essentially by the send/receive family of MPI functions, has been evolving towards \textbf{hybrid models}. In a hybrid model, the threads or processes within a node communicate directly with each other by sharing the node's memory with no traditional send/receive involved, while the inter-node communication is indeed handled using the send/receive traditional approach. Currently, one of the most common hybrid programing model for multicore combines MPI and OpenMP, denoted here as MPI+OpenMP.



\begin{comment}






Today's trend of having 
an increasing number of cores per CPU (computing component, socket,chip) have created the need for approaches that combines the best of this two programing models: 


Almost all chips are multicore these days


 into what is nowadays known as Hybrid programming models.


It describes parallelism between processes having separate memory space; 

a 


in contrast to 

\emph{thread} parallelism which provides a shared memory model within a process. 

OpenMP and Pthreads are common examples of the \emph{thread} parallelism model.




Scale of machines to come encourage the use of different programming models to address issues such as

\begin{itemize} 

\item Declining memory per core

\item Multiple threads/core

\item Load balance

\item Algorithmic issues


\end{itemize}
\end{comment}



\medskip

The latest MPI standard (MPI-3) added several functions to allow processes that exist within a node to directly allocate and share the node's memory, in a way similar to OpenMP. This update to the MPI standard permits a new hybrid model in which traditional MPI is still used for inter-node communication while the new shared memory capabilities of MPI (MPI$_{sm}$) are used for the communication between the processes inside the node. In this document, this new hybrid model is denoted as MPI+MPI$_{sm}$ to distinguish it from MPI+OpenMP.

\medskip


Additionally, this new shared memory programing capabilities of MPI-3 (MPI$_{sm}$) could turn it into a viable alternative to OpenMP as a shared memory programing model.
 
\medskip


The objective of this guide is to describe, via examples, the new shared memory capabilities of MPI-3 and to explore its viability in the context of \textbf{share memory programing model} and then as a \textbf{hybrid programing model}.


\subsection*{Systems and compiler used}

All the computer utilized in this study corresponds to systems having more than one socket (ccNUMA nodes). These kind of architecture is ubiquitous nowadays. On those systems all the memory is accessible by all the cores, however the memory is distributed between the sockets. The efficient programing of these systems requires awareness of issues such as memory effects and thread/process placement.


Three compilers were used in the results presented in the following sections: the GNU \textbf{gcc}  version 7.3.0 (OpenMP 4.5), the Intel \textbf{icc} version 18.0.1 (OpenMP 5.0) and the  PGI \textbf{pgcc} version 18.3 (OpenMP 4.0). For the MPI implementation used, \textbf{gcc} and the \textbf{pgi} compliers both use \textbf{open-mpi} while the \textbf{intel} compiler used it own mpi implementation.



\subsection*{Organization}


\medskip

The lessons learned during the development of this work will be remarked and commented in the coming sections. This document is organized as follows:

\begin{itemize} 
\item Section 2 describes a small set of MPI$_{sm}$ functions required to use a shared memory programing model. %, denoted in this guide as MPI$_{sm}$.

\item Section 3 presents and compare results of the Stream benchmark obtained using MPI$_{sm}$ and OpenMP.

\item In Section 4, two computer programs are used to compare the performance of MPI$_{sm}$ and OpenMP.

\item Section 5 describes a small set of the MPI functions that can be used to effectively implement hybrid programs.

\item In Section 6, one computer program is used to compare the performance of two hybrid programing models: MPI+MPI$_{sm}$ and MPI+OpenMP.

%\item Section 5 describe some additional functions and techniques required by hybrid programing. Additionally it compares two hybrid programing model: MPI+MPI$_{sm}$ and MPI+OpenMP.

\end{itemize}






